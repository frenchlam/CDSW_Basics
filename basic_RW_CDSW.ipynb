{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading / Writing files from HDFS in CDSW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-09 10:57:37,634 INFO  [main] fs.TrashPolicyDefault (TrashPolicyDefault.java:moveToTrash(182)) - Moved: 'hdfs://mlamairesse-1.vpc.cloudera.com:8020/tmp/1988.csv.bz2' to trash at: hdfs://mlamairesse-1.vpc.cloudera.com:8020/user/systest/.Trash/Current/tmp/1988.csv.bz2\n",
      "2020-03-09 10:57:39,983 INFO  [main] fs.TrashPolicyDefault (TrashPolicyDefault.java:moveToTrash(182)) - Moved: 'hdfs://mlamairesse-1.vpc.cloudera.com:8020/tmp/airports.csv' to trash at: hdfs://mlamairesse-1.vpc.cloudera.com:8020/user/systest/.Trash/Current/tmp/airports.csv\n",
      "2020-03-09 10:57:42,336 INFO  [main] fs.TrashPolicyDefault (TrashPolicyDefault.java:moveToTrash(182)) - Moved: 'hdfs://mlamairesse-1.vpc.cloudera.com:8020/tmp/airports_python.csv' to trash at: hdfs://mlamairesse-1.vpc.cloudera.com:8020/user/systest/.Trash/Current/tmp/airports_python.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n",
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n",
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "## Cleanup - delete file if exits \n",
    "hdfs dfs -rm -f /tmp/1988.csv.bz2\n",
    "hdfs dfs -rm -f /tmp/airports.csv\n",
    "hdfs dfs -rm -f /tmp/airports_python.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1 : Use HDF CLI (from the command line or python)\n",
    "### Copy data To and From the local file system and work from there\n",
    "#### **Applicable to:** **\"small and medium\"** datasets ; ie : small enough to be easily managed/processed locally. <br>\n",
    "**NOTE:** To be used in the context of **sensitive data and/or data that requiere strong governance** as it creates a break in the data chain of custody / security. <br> \n",
    "This is not a best practice in terms of Data Management/Governance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Copy data TO HDFS from local file system \n",
    "#### Using the command line (works in workbench as well)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6 items\n",
      "d---------   - hdfs    supergroup          0 2020-03-09 10:59 /tmp/.cloudera_health_monitoring_canary_files\n",
      "-rw-r--r--   2 systest supergroup     244438 2020-03-09 10:59 /tmp/airports.csv\n",
      "drwxrwxrwx   - hive    supergroup          0 2020-03-06 18:30 /tmp/hive\n",
      "drwxrwxrwt   - mapred  hadoop              0 2020-03-06 18:24 /tmp/logs\n",
      "drwxr-xr-x   - systest supergroup          0 2020-03-09 00:07 /tmp/wine_pred\n",
      "drwxr-xr-x   - systest supergroup          0 2020-03-09 00:08 /tmp/wine_pred_hive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n",
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "### Copy to HDFS - using HDFS client - from Bash\n",
    "# NOTE: Only functional from a Bash script or an interactive session \n",
    "hdfs dfs -copyFromLocal -f /home/cdsw/airlines/airports/airports.csv /tmp/\n",
    "hdfs dfs -ls /tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Python - Write data to the HDFS CLI (via subprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### local and HDFS Paths \n",
    "local_path=\"/home/cdsw/airlines/airports/airports.csv\"\n",
    "hdfs_path=\"/tmp/airports_python.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n",
      "Found 7 items\n",
      "d---------   - hdfs    supergroup          0 2020-03-09 11:00 /tmp/.cloudera_health_monitoring_canary_files\n",
      "-rw-r--r--   2 systest supergroup     244438 2020-03-09 10:59 /tmp/airports.csv\n",
      "-rw-r--r--   2 systest supergroup     244438 2020-03-09 11:00 /tmp/airports_python.csv\n",
      "drwxrwxrwx   - hive    supergroup          0 2020-03-06 18:30 /tmp/hive\n",
      "drwxrwxrwt   - mapred  hadoop              0 2020-03-06 18:24 /tmp/logs\n",
      "drwxr-xr-x   - systest supergroup          0 2020-03-09 00:07 /tmp/wine_pred\n",
      "drwxr-xr-x   - systest supergroup          0 2020-03-09 00:08 /tmp/wine_pred_hive\n"
     ]
    }
   ],
   "source": [
    "### Copy to HDFS - Using HDFS Client - From Python ( using subprocess )\n",
    "from subprocess import Popen, PIPE\n",
    "import sys\n",
    "\n",
    "def hdfs_write(local_path,hdfs_path):\n",
    "    ### Copy to HDFS - Python (using subprocess)\n",
    "    from subprocess import Popen, PIPE\n",
    "    put = Popen([\"hadoop\",\"fs\",\"-put\",\"-f\", local_path, hdfs_path], stdin=PIPE,stdout=PIPE,stderr=PIPE)\n",
    "    stdout, stderr = put.communicate()\n",
    "    \n",
    "    ## Error handling\n",
    "    if put.returncode != 0: \n",
    "        raise IOError(stderr)\n",
    "\n",
    "hdfs_write(local_path,hdfs_path)\n",
    "\n",
    "#Show hdfs path\n",
    "!hdfs dfs -ls /tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Tip: Use a pipe to download files directly to HDFS (command line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 items\n",
      "d---------   - hdfs    supergroup          0 2020-03-09 11:02 /tmp/.cloudera_health_monitoring_canary_files\n",
      "-rw-r--r--   2 systest supergroup   49499025 2020-03-09 11:02 /tmp/1988.csv.bz2\n",
      "-rw-r--r--   2 systest supergroup     244438 2020-03-09 10:59 /tmp/airports.csv\n",
      "-rw-r--r--   2 systest supergroup     244438 2020-03-09 11:00 /tmp/airports_python.csv\n",
      "drwxrwxrwx   - hive    supergroup          0 2020-03-06 18:30 /tmp/hive\n",
      "drwxrwxrwt   - mapred  hadoop              0 2020-03-06 18:24 /tmp/logs\n",
      "drwxr-xr-x   - systest supergroup          0 2020-03-09 00:07 /tmp/wine_pred\n",
      "drwxr-xr-x   - systest supergroup          0 2020-03-09 00:08 /tmp/wine_pred_hive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r",
      "  0 47.2M    0 85614    0     0  44753      0  0:18:26  0:00:01  0:18:25 44730\r",
      "  3 47.2M    3 1885k    0     0   766k      0  0:01:03  0:00:02  0:01:01  766k\r",
      " 25 47.2M   25 12.0M    0     0  3615k      0  0:00:13  0:00:03  0:00:10 3614k\r",
      " 50 47.2M   50 24.0M    0     0  5576k      0  0:00:08  0:00:04  0:00:04 5576k\r",
      " 73 47.2M   73 34.5M    0     0  6486k      0  0:00:07  0:00:05  0:00:02 7024k\r",
      " 95 47.2M   95 45.0M    0     0  7200k      0  0:00:06  0:00:06 --:--:-- 10.0M\r",
      "100 47.2M  100 47.2M    0     0  7383k      0  0:00:06  0:00:06 --:--:-- 11.0M\n",
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "## NOTE: Only functional from a Bash script or an interactive session \n",
    "export DOWNLOAD_LINK='https://mlamairesse.s3-eu-west-1.amazonaws.com/Airlines_Dataset/1988.csv.bz2'\n",
    "curl $DOWNLOAD_LINK | hadoop fs -put - /tmp/1988.csv.bz2\n",
    "\n",
    "hdfs dfs -ls /tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Copy data FROM HDFS to the local file sytem \n",
    "#### Command line "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n",
      "get: `/home/cdsw/airlines/from_hdfs/airports.csv': File exists\n",
      "total 240K\n",
      "-rw-r--r-- 1 cdsw cdsw 239K Mar  8 20:55 airports.csv\n"
     ]
    }
   ],
   "source": [
    "## Copy the file from HDFS\n",
    "!hdfs dfs -get airlines/airports/airports.csv /home/cdsw/airlines/from_hdfs/\n",
    "#show content of directory\n",
    "!ls -lh ~/airlines/from_hdfs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Read Data FROM HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the console ( get a feel for what data looks like ) \n",
    "**NOTE:** using `-text` rather than `-cat` allows reading from compressed files (zip,gz,bz2,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n",
      "\"iata\",\"airport\",\"city\",\"state\",\"country\",\"lat\",\"long\"\n",
      "\"00M\",\"Thigpen \",\"Bay Springs\",\"MS\",\"USA\",31.95376472,-89.23450472\n",
      "\"00R\",\"Livingston Municipal\",\"Livingston\",\"TX\",\"USA\",30.68586111,-95.01792778\n",
      "\"00V\",\"Meadow Lake\",\"Colorado Springs\",\"CO\",\"USA\",38.94574889,-104.5698933\n",
      "\"01G\",\"Perry-Warsaw\",\"Perry\",\"NY\",\"USA\",42.74134667,-78.05208056\n",
      "text: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -text airlines/airports/airports.csv | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Python - Read data from HDFS using the HDFS CLI (via subprocess) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"iata\",\"airport\",\"city\",\"state\",\"country\",\"lat\",\"l'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from subprocess import Popen, PIPE\n",
    "import sys\n",
    "\n",
    "def hdfs_read(hdfs_path):\n",
    "    proc = Popen(['hadoop', 'fs', '-text' , hdfs_path], stdout=PIPE, stderr=PIPE, universal_newlines=False)\n",
    "    stdout, stderr = proc.communicate()\n",
    "    \n",
    "    if proc.returncode != 0:\n",
    "        if 'No such file or directory' in stderr.decode('utf-8'):\n",
    "            raise FileNotFoundError('No such file or directory: {}'.format(local_path))\n",
    "        else : \n",
    "            raise IOError(stderr)\n",
    "    \n",
    "    return stdout\n",
    "    #Return a \"bytes\" object ; \n",
    "\n",
    "HDFS_file = hdfs_read(\"airlines/airports/airports.csv\").decode('utf-8') #decode as a string - File in Memory\n",
    "HDFS_file[0:50] # get first 50 char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Pandas - Read data from HDFS using HDFS CLI (via subprocess) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iata</th>\n",
       "      <th>airport</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00M</td>\n",
       "      <td>Thigpen</td>\n",
       "      <td>Bay Springs</td>\n",
       "      <td>MS</td>\n",
       "      <td>USA</td>\n",
       "      <td>31.953765</td>\n",
       "      <td>-89.234505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00R</td>\n",
       "      <td>Livingston Municipal</td>\n",
       "      <td>Livingston</td>\n",
       "      <td>TX</td>\n",
       "      <td>USA</td>\n",
       "      <td>30.685861</td>\n",
       "      <td>-95.017928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00V</td>\n",
       "      <td>Meadow Lake</td>\n",
       "      <td>Colorado Springs</td>\n",
       "      <td>CO</td>\n",
       "      <td>USA</td>\n",
       "      <td>38.945749</td>\n",
       "      <td>-104.569893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01G</td>\n",
       "      <td>Perry-Warsaw</td>\n",
       "      <td>Perry</td>\n",
       "      <td>NY</td>\n",
       "      <td>USA</td>\n",
       "      <td>42.741347</td>\n",
       "      <td>-78.052081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01J</td>\n",
       "      <td>Hilliard Airpark</td>\n",
       "      <td>Hilliard</td>\n",
       "      <td>FL</td>\n",
       "      <td>USA</td>\n",
       "      <td>30.688012</td>\n",
       "      <td>-81.905944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  iata               airport              city state country        lat  \\\n",
       "0  00M              Thigpen        Bay Springs    MS     USA  31.953765   \n",
       "1  00R  Livingston Municipal        Livingston    TX     USA  30.685861   \n",
       "2  00V           Meadow Lake  Colorado Springs    CO     USA  38.945749   \n",
       "3  01G          Perry-Warsaw             Perry    NY     USA  42.741347   \n",
       "4  01J      Hilliard Airpark          Hilliard    FL     USA  30.688012   \n",
       "\n",
       "         long  \n",
       "0  -89.234505  \n",
       "1  -95.017928  \n",
       "2 -104.569893  \n",
       "3  -78.052081  \n",
       "4  -81.905944  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### NOTE: Can be read directly from Pandas with a bit of transformation: \n",
    "### Pandas accepts file, path or StringIO object \n",
    "### https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html\n",
    "from io import StringIO\n",
    "import pandas as pd \n",
    "airlines_pd_df = pd.read_csv(StringIO(hdfs_read(\"airlines/airports/airports.csv\").decode('utf-8')),\n",
    "                             sep=',', delimiter=None, header='infer')\n",
    "airlines_pd_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2 - Read/Write using spark\n",
    "### **Applicable to:**  All datasets and large ones in particular <br> \n",
    "Using spark, allows us to use the data without having to copy first. It's much cleaner in terms of chain of custody <br>\n",
    "**NOTE:** For large dataset, it also allows us to do filtering, pre-processing and filtering in a distributed manner which is much more efficient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Reading data from HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Start the spark session\n",
    "Custom session configuration can be defined either in the session parameters as below OR\n",
    "inside a `spark-defaults.conf` file stored at the root of the project (in which case the configs become project wide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .master(\"yarn\")\\\n",
    "    .appName(\"Airline\")\\\n",
    "    .config(\"spark.executor.memory\",\"2g\")\\\n",
    "    .config(\"spark.executor.cores\",\"2\")\\\n",
    "    .config(\"spark.driver.memory\",\"2g\")\\\n",
    "    .config(\"spark.executor.instances\",\"2\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"http://spark-qvn3pascdozh0ueg.mlamairesse-4.vpc.cloudera.com\" target=\"_blank\" >Spark UI</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Adding a link to the Spark UI for demo purposes\n",
    "## Also available in the session tab\n",
    "from IPython.core.display import HTML\n",
    "import os\n",
    "HTML('<a href=\"http://spark-{}.{}\" target=\"_blank\" >Spark UI</a>'.\\\n",
    "    format(os.getenv(\"CDSW_ENGINE_ID\"),os.getenv(\"CDSW_DOMAIN\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Read Data - CSV file stored on HDFS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- iata: string (nullable = true)\n",
      " |-- airport: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path='airlines/airports/airports.csv' #HDFS location (relative path)\n",
    "\n",
    "airports_df = spark.read.csv(\n",
    "    path=path,\n",
    "    header=True,\n",
    "    sep=',',\n",
    "    inferSchema=True,\n",
    "    nullValue=None\n",
    ")\n",
    "airports_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** : in the above example, I'm infering the schema from the file. <br>\n",
    "In a re-usable script, it's actually good practice to set the schema to prevent erroneous type casting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+----------------+-----+-------+-----------+------------+\n",
      "|iata|             airport|            city|state|country|        lat|        long|\n",
      "+----+--------------------+----------------+-----+-------+-----------+------------+\n",
      "| 00M|            Thigpen |     Bay Springs|   MS|    USA|31.95376472|-89.23450472|\n",
      "| 00R|Livingston Municipal|      Livingston|   TX|    USA|30.68586111|-95.01792778|\n",
      "| 00V|         Meadow Lake|Colorado Springs|   CO|    USA|38.94574889|-104.5698933|\n",
      "| 01G|        Perry-Warsaw|           Perry|   NY|    USA|42.74134667|-78.05208056|\n",
      "| 01J|    Hilliard Airpark|        Hilliard|   FL|    USA| 30.6880125|-81.90594389|\n",
      "+----+--------------------+----------------+-----+-------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "path='airlines/airports/airports.csv' #HDFS location\n",
    "schema = StructType([StructField(\"iata\", StringType(), True),\n",
    "                     StructField(\"airport\", StringType(), True),\n",
    "                     StructField(\"city\", StringType(), True),\n",
    "                     StructField(\"state\", StringType(), True),\n",
    "                     StructField(\"country\", StringType(), True),\n",
    "                     StructField(\"lat\",  DoubleType(), True),\n",
    "                     StructField(\"long\",  DoubleType(), True)\n",
    "                    ])\n",
    "\n",
    "airports_df = spark.read.csv(\n",
    "    path=path,\n",
    "    schema=schema,\n",
    "    header=True,\n",
    "    sep=',',\n",
    "    nullValue=None) \n",
    "airports_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 (optional) Transform data to Pandas Dataframe\n",
    "#### Once converted **ALL DATA will be brought locally** and distributed processing ends \n",
    "* **Applicable to : SMALL to MEDIUM size datasets** - ie : datasets that can be easily managed/processed locally\n",
    "* When working with **LARGE datasets** :  **data should be sampled** before bringing it locally\n",
    "\n",
    "> **Good Practice**:  Spark context should be stopped `spark.stop()` to release cluster ressources once data is copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3376 entries, 0 to 3375\n",
      "Data columns (total 7 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   iata     3376 non-null   object \n",
      " 1   airport  3376 non-null   object \n",
      " 2   city     3376 non-null   object \n",
      " 3   state    3376 non-null   object \n",
      " 4   country  3376 non-null   object \n",
      " 5   lat      3376 non-null   float64\n",
      " 6   long     3376 non-null   float64\n",
      "dtypes: float64(2), object(5)\n",
      "memory usage: 184.8+ KB\n"
     ]
    }
   ],
   "source": [
    "#without sampling\n",
    "import pandas \n",
    "airport_pandas_df = airports_df.toPandas()\n",
    "airport_pandas_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1111 entries, 0 to 1110\n",
      "Data columns (total 7 columns):\n",
      "iata       1111 non-null object\n",
      "airport    1111 non-null object\n",
      "city       1111 non-null object\n",
      "state      1111 non-null object\n",
      "country    1111 non-null object\n",
      "lat        1111 non-null float64\n",
      "long       1111 non-null float64\n",
      "dtypes: float64(2), object(5)\n",
      "memory usage: 60.9+ KB\n"
     ]
    }
   ],
   "source": [
    "#with sampling\n",
    "sample_pandas_df = airports_df.sample(1/3,seed=30).toPandas()\n",
    "sample_pandas_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write data from PANDAS to HDFS - Using Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Read data locally using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iata</th>\n",
       "      <th>airport</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>ADK</td>\n",
       "      <td>Adak</td>\n",
       "      <td>Adak</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>51.877964</td>\n",
       "      <td>-176.646031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>AKK</td>\n",
       "      <td>Akhiok</td>\n",
       "      <td>Akhiok</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>56.938691</td>\n",
       "      <td>-154.182556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3363</th>\n",
       "      <td>Z13</td>\n",
       "      <td>Akiachak</td>\n",
       "      <td>Akiachak</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>60.904532</td>\n",
       "      <td>-161.420910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817</th>\n",
       "      <td>AKI</td>\n",
       "      <td>Akiak</td>\n",
       "      <td>Akiak</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>60.904812</td>\n",
       "      <td>-161.227019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>KQA</td>\n",
       "      <td>Akutan SPB</td>\n",
       "      <td>Akutan</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>54.132467</td>\n",
       "      <td>-165.785311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     iata     airport      city state country        lat        long\n",
       "776   ADK        Adak      Adak    AK     USA  51.877964 -176.646031\n",
       "818   AKK      Akhiok    Akhiok    AK     USA  56.938691 -154.182556\n",
       "3363  Z13    Akiachak  Akiachak    AK     USA  60.904532 -161.420910\n",
       "817   AKI       Akiak     Akiak    AK     USA  60.904812 -161.227019\n",
       "1994  KQA  Akutan SPB    Akutan    AK     USA  54.132467 -165.785311"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## read from pandas\n",
    "import pandas as pd\n",
    "airlines_pd_df = pd.read_csv(\"/home/cdsw/airlines/airports/airports.csv\",sep=',', delimiter=None, header='infer')\n",
    "airlines_pd_df.sort_values(by=['state','airport'],inplace=True) # ordering to keep same visulisation order as below\n",
    "airlines_pd_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Transform Pandas DataFrame to Spark DataFrame\n",
    "With spark 2.3 and up, integration with Pandas has been reinforced notably with the use of Arrow for faster data transfers [https://issues.apache.org/jira/browse/SPARK-20791]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+--------+-----+-------+-----------+-------------------+\n",
      "|iata|   airport|    city|state|country|        lat|               long|\n",
      "+----+----------+--------+-----+-------+-----------+-------------------+\n",
      "| ADK|      Adak|    Adak|   AK|    USA|51.87796389|       -176.6460306|\n",
      "| AKK|    Akhiok|  Akhiok|   AK|    USA|56.93869083|       -154.1825556|\n",
      "| Z13|  Akiachak|Akiachak|   AK|    USA|60.90453167|-161.42091000000002|\n",
      "| AKI|     Akiak|   Akiak|   AK|    USA|60.90481194|       -161.2270189|\n",
      "| KQA|Akutan SPB|  Akutan|   AK|    USA|54.13246694|       -165.7853111|\n",
      "+----+----------+--------+-----+-------+-----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# #(optional) Enable Arrow-based optimised columnar data transfers ; Note : still marked as experimental\n",
    "# #PyArrow must be intalled on all spark nodes\n",
    "# #https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html#ensure-pyarrow-installed\n",
    "# spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\") # Note : Compatible only with pyarrow 0.8.0 \n",
    "\n",
    "#(optional) good practice to define schema to prevent any type casting errors\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([StructField(\"iata\", StringType(), True),\n",
    "                     StructField(\"airport\", StringType(), True),\n",
    "                     StructField(\"city\", StringType(), True),\n",
    "                     StructField(\"state\", StringType(), True),\n",
    "                     StructField(\"country\", StringType(), True),\n",
    "                     StructField(\"lat\",  DoubleType(), True),\n",
    "                     StructField(\"long\",  DoubleType(), True)\n",
    "                    ])\n",
    "\n",
    "spark_df=spark.createDataFrame(airlines_pd_df,schema=schema)\n",
    "spark_df.orderBy(['state','airport']).show(5) # ordering to keep same visulisation order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "field city: Can not merge type <class 'pyspark.sql.types.StringType'> and <class 'pyspark.sql.types.DoubleType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b32a090e7a81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# example of a typecasting error...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mspark_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mairlines_pd_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_createFromLocal\u001b[0;34m(self, data, schema)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mstruct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferSchemaFromList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/sql/session.py\u001b[0m in \u001b[0;36m_inferSchemaFromList\u001b[0;34m(self, data, names)\u001b[0m\n\u001b[1;32m    357\u001b[0m             warnings.warn(\"inferring schema from dict is deprecated,\"\n\u001b[1;32m    358\u001b[0m                           \"please use pyspark.sql.Row instead\")\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_merge_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_infer_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Some of types cannot be determined after inferring\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_merge_type\u001b[0;34m(a, b, name)\u001b[0m\n\u001b[1;32m   1099\u001b[0m         fields = [StructField(f.name, _merge_type(f.dataType, nfs.get(f.name, NullType()),\n\u001b[1;32m   1100\u001b[0m                                                   name=new_name(f.name)))\n\u001b[0;32m-> 1101\u001b[0;31m                   for f in a.fields]\n\u001b[0m\u001b[1;32m   1102\u001b[0m         \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnfs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1099\u001b[0m         fields = [StructField(f.name, _merge_type(f.dataType, nfs.get(f.name, NullType()),\n\u001b[1;32m   1100\u001b[0m                                                   name=new_name(f.name)))\n\u001b[0;32m-> 1101\u001b[0;31m                   for f in a.fields]\n\u001b[0m\u001b[1;32m   1102\u001b[0m         \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnfs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/cloudera/parcels/CDH/lib/spark/python/pyspark/sql/types.py\u001b[0m in \u001b[0;36m_merge_type\u001b[0;34m(a, b, name)\u001b[0m\n\u001b[1;32m   1092\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m         \u001b[0;31m# TODO: type cast (such as int -> long)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can not merge type %s and %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m     \u001b[0;31m# same type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: field city: Can not merge type <class 'pyspark.sql.types.StringType'> and <class 'pyspark.sql.types.DoubleType'>"
     ]
    }
   ],
   "source": [
    "# example of a typecasting error... \n",
    "spark_df=spark.createDataFrame(airlines_pd_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Write to HDFS - using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: log4j.properties is not found. HADOOP_CONF_DIR may be incomplete.\n",
      "Found 3 items\n",
      "-rw-r--r--   2 systest supergroup          0 2020-01-07 11:53 /tmp/airlines/_SUCCESS\n",
      "-rw-r--r--   2 systest supergroup      72942 2020-01-07 11:53 /tmp/airlines/part-00000-cc364f67-10e5-46e6-8468-d85f79b60ef3-c000.snappy.parquet\n",
      "-rw-r--r--   2 systest supergroup      72504 2020-01-07 11:53 /tmp/airlines/part-00001-cc364f67-10e5-46e6-8468-d85f79b60ef3-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "## It's good practice to restructure data before writing to HDFS : Spark write a file by partition. \n",
    "## this can lead to lots of small files which is counterproductive both for read and write. \n",
    "## Re-organize data using the \"coalesce\" function to define the number of files to be saved\n",
    "spark_df.coalesce(2).write.parquet('/tmp/airlines/', mode='overwrite')\n",
    "\n",
    "!hdfs dfs -ls /tmp/airlines/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Write Data to Hive - using Spark\n",
    "Spark to hive integration makes it very easy to interact with the cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Note : Ordering on write can help optimise reads later on. \n",
    "spark_df.orderBy(['state','airport']).coalesce(2)\\\n",
    "    .write.format('parquet').mode(\"overwrite\")\\\n",
    "    .saveAsTable('default.airports')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Read Data from Hive \n",
    "All hive configurations are already injected into spark.  Therefore Hive can be called directly using a spark sql context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Read data from hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+-----------+\n",
      "|database|         tableName|isTemporary|\n",
      "+--------+------------------+-----------+\n",
      "| default|          airports|      false|\n",
      "| default|         customers|      false|\n",
      "| default|         sample_07|      false|\n",
      "| default|         sample_08|      false|\n",
      "| default|          web_logs|      false|\n",
      "| default|        wineds_ext|      false|\n",
      "| default|wineds_ext_nolabel|      false|\n",
      "+--------+------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use *spark.sql(\"some SQL statement\")* to execture queries against hive\n",
    "sql_statement = '''show tables in default'''\n",
    "spark.sql(sql_statement).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+--------------+-----+-------+-----------+-------------------+\n",
      "|iata|   airport|          city|state|country|        lat|               long|\n",
      "+----+----------+--------------+-----+-------+-----------+-------------------+\n",
      "| ADK|      Adak|          Adak|   AK|    USA|51.87796389|       -176.6460306|\n",
      "| AKK|    Akhiok|        Akhiok|   AK|    USA|56.93869083|       -154.1825556|\n",
      "| Z13|  Akiachak|      Akiachak|   AK|    USA|60.90453167|-161.42091000000002|\n",
      "| AKI|     Akiak|         Akiak|   AK|    USA|60.90481194|       -161.2270189|\n",
      "| KQA|Akutan SPB|        Akutan|   AK|    USA|54.13246694|       -165.7853111|\n",
      "| AUK|  Alakanuk|      Alakanuk|   AK|    USA|62.68004417|       -164.6599253|\n",
      "| 5A8| Aleknagik|     Aleknagik|   AK|    USA|59.28256167|       -158.6176725|\n",
      "| 6A8| Allakaket|     Allakaket|   AK|    USA|66.55194444|-152.62222219999998|\n",
      "| BIG| Allen AAF|Delta Junction|   AK|    USA|63.99454722|       -145.7216417|\n",
      "| AFM|    Ambler|        Ambler|   AK|    USA|67.10610472|-157.85362030000002|\n",
      "+----+----------+--------------+-----+-------+-----------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ex. read table\n",
    "sql_statement = '''select * from default.airports where state = \"AK\" '''\n",
    "airports_df = spark.sql(sql_statement)\n",
    "airports_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 (optional) convert to pandas dataframe (see 1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iata</th>\n",
       "      <th>airport</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADK</td>\n",
       "      <td>Adak</td>\n",
       "      <td>Adak</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>51.877964</td>\n",
       "      <td>-176.646031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AKK</td>\n",
       "      <td>Akhiok</td>\n",
       "      <td>Akhiok</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>56.938691</td>\n",
       "      <td>-154.182556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Z13</td>\n",
       "      <td>Akiachak</td>\n",
       "      <td>Akiachak</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>60.904532</td>\n",
       "      <td>-161.420910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AKI</td>\n",
       "      <td>Akiak</td>\n",
       "      <td>Akiak</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>60.904812</td>\n",
       "      <td>-161.227019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KQA</td>\n",
       "      <td>Akutan SPB</td>\n",
       "      <td>Akutan</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>54.132467</td>\n",
       "      <td>-165.785311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  iata     airport      city state country        lat        long\n",
       "0  ADK        Adak      Adak    AK     USA  51.877964 -176.646031\n",
       "1  AKK      Akhiok    Akhiok    AK     USA  56.938691 -154.182556\n",
       "2  Z13    Akiachak  Akiachak    AK     USA  60.904532 -161.420910\n",
       "3  AKI       Akiak     Akiak    AK     USA  60.904812 -161.227019\n",
       "4  KQA  Akutan SPB    Akutan    AK     USA  54.132467 -165.785311"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#(OPTIONAL) convert to pandas \n",
    "airlines_pd_df = airports_df.toPandas()\n",
    "airlines_pd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop() ## Release spark ressources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***NOTE:*** Pandas Dataframe is still available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iata</th>\n",
       "      <th>airport</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00M</td>\n",
       "      <td>Thigpen</td>\n",
       "      <td>Bay Springs</td>\n",
       "      <td>MS</td>\n",
       "      <td>USA</td>\n",
       "      <td>31.953765</td>\n",
       "      <td>-89.234505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00R</td>\n",
       "      <td>Livingston Municipal</td>\n",
       "      <td>Livingston</td>\n",
       "      <td>TX</td>\n",
       "      <td>USA</td>\n",
       "      <td>30.685861</td>\n",
       "      <td>-95.017928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00V</td>\n",
       "      <td>Meadow Lake</td>\n",
       "      <td>Colorado Springs</td>\n",
       "      <td>CO</td>\n",
       "      <td>USA</td>\n",
       "      <td>38.945749</td>\n",
       "      <td>-104.569893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01G</td>\n",
       "      <td>Perry-Warsaw</td>\n",
       "      <td>Perry</td>\n",
       "      <td>NY</td>\n",
       "      <td>USA</td>\n",
       "      <td>42.741347</td>\n",
       "      <td>-78.052081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01J</td>\n",
       "      <td>Hilliard Airpark</td>\n",
       "      <td>Hilliard</td>\n",
       "      <td>FL</td>\n",
       "      <td>USA</td>\n",
       "      <td>30.688012</td>\n",
       "      <td>-81.905944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  iata               airport              city state country        lat  \\\n",
       "0  00M              Thigpen        Bay Springs    MS     USA  31.953765   \n",
       "1  00R  Livingston Municipal        Livingston    TX     USA  30.685861   \n",
       "2  00V           Meadow Lake  Colorado Springs    CO     USA  38.945749   \n",
       "3  01G          Perry-Warsaw             Perry    NY     USA  42.741347   \n",
       "4  01J      Hilliard Airpark          Hilliard    FL     USA  30.688012   \n",
       "\n",
       "         long  \n",
       "0  -89.234505  \n",
       "1  -95.017928  \n",
       "2 -104.569893  \n",
       "3  -78.052081  \n",
       "4  -81.905944  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airport_pandas_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Read Directly from HIVE - over JDBC \n",
    "**NOTE:** Must know Hive host and port (default 10000) information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To Do before running** : Replace **hive_host** and **hive_port** information with Hive host information pertinent to the cluster used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive_host='mlamairesse-1.vpc.cloudera.com'\n",
    "hive_port=10000 #default 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 using PyHive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyhive.hive.Connection"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a connection\n",
    "\n",
    "# doc for connection string : \n",
    "#connection=hive.Connection(host=<hive_host>, port=<hive_port (default=10000)>, auth='KERBEROS', kerberos_service_name='hive')\n",
    "from pyhive import hive\n",
    "\n",
    "connection=hive.Connection(host=hive_host, port=hive_port, auth='KERBEROS', kerberos_service_name='hive')\n",
    "type(connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ADK', 'Adak', 'Adak', 'AK', 'USA', 51.87796389, -176.6460306)\n",
      "('AKK', 'Akhiok', 'Akhiok', 'AK', 'USA', 56.93869083, -154.1825556)\n",
      "('Z13', 'Akiachak', 'Akiachak', 'AK', 'USA', 60.90453167, -161.42091000000002)\n",
      "('AKI', 'Akiak', 'Akiak', 'AK', 'USA', 60.90481194, -161.2270189)\n",
      "('KQA', 'Akutan SPB', 'Akutan', 'AK', 'USA', 54.13246694, -165.7853111)\n",
      "('AUK', 'Alakanuk', 'Alakanuk', 'AK', 'USA', 62.68004417, -164.6599253)\n",
      "('5A8', 'Aleknagik', 'Aleknagik', 'AK', 'USA', 59.28256167, -158.6176725)\n",
      "('6A8', 'Allakaket', 'Allakaket', 'AK', 'USA', 66.55194444, -152.62222219999998)\n",
      "('BIG', 'Allen AAF', 'Delta Junction', 'AK', 'USA', 63.99454722, -145.7216417)\n",
      "('AFM', 'Ambler', 'Ambler', 'AK', 'USA', 67.10610472, -157.85362030000002)\n",
      "('AKP', 'Anaktuvuk Pass', 'Anaktuvuk Pass', 'AK', 'USA', 68.1343225, -151.74168)\n",
      "('AGN', 'Angoon SPB', 'Angoon', 'AK', 'USA', 57.50355528, -134.5850939)\n",
      "('ANI', 'Aniak', 'Aniak', 'AK', 'USA', 61.58159694, -159.5430428)\n",
      "('ANV', 'Anvik', 'Anvik', 'AK', 'USA', 62.64858333, -160.1898889)\n",
      "('ARC', 'Arctic Village', 'Arctic Village', 'AK', 'USA', 68.11608083, -145.5761114)\n",
      "('AKA', 'Atka', 'Atka', 'AK', 'USA', 52.22034833, -174.2063503)\n",
      "('4A2', 'Atmautluak', 'Atmautluak', 'AK', 'USA', 60.86674556, -162.2731389)\n",
      "('ATK', 'Atqasuk', 'Atqasuk', 'AK', 'USA', 70.46727611, -157.4357361)\n",
      "('BNF', 'Baranof Warm Springs SPB', 'Baranof Warm Springs', 'AK', 'USA', 57.08882583, -134.83314140000002)\n",
      "('BTI', 'Barter Island ', 'Kaktovik', 'AK', 'USA', 70.13390278, -143.5770444)\n"
     ]
    }
   ],
   "source": [
    "# Basic reading - using cursor\n",
    "cur = connection.cursor()\n",
    "cur.execute('Select * from default.airports LIMIT 20')\n",
    "results = cur.fetchall()\n",
    "for line in results : \n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airports.iata</th>\n",
       "      <th>airports.airport</th>\n",
       "      <th>airports.city</th>\n",
       "      <th>airports.state</th>\n",
       "      <th>airports.country</th>\n",
       "      <th>airports.lat</th>\n",
       "      <th>airports.long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADK</td>\n",
       "      <td>Adak</td>\n",
       "      <td>Adak</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>51.877964</td>\n",
       "      <td>-176.646031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AKK</td>\n",
       "      <td>Akhiok</td>\n",
       "      <td>Akhiok</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>56.938691</td>\n",
       "      <td>-154.182556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Z13</td>\n",
       "      <td>Akiachak</td>\n",
       "      <td>Akiachak</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>60.904532</td>\n",
       "      <td>-161.420910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AKI</td>\n",
       "      <td>Akiak</td>\n",
       "      <td>Akiak</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>60.904812</td>\n",
       "      <td>-161.227019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KQA</td>\n",
       "      <td>Akutan SPB</td>\n",
       "      <td>Akutan</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>54.132467</td>\n",
       "      <td>-165.785311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airports.iata airports.airport airports.city airports.state  \\\n",
       "0           ADK             Adak          Adak             AK   \n",
       "1           AKK           Akhiok        Akhiok             AK   \n",
       "2           Z13         Akiachak      Akiachak             AK   \n",
       "3           AKI            Akiak         Akiak             AK   \n",
       "4           KQA       Akutan SPB        Akutan             AK   \n",
       "\n",
       "  airports.country  airports.lat  airports.long  \n",
       "0              USA     51.877964    -176.646031  \n",
       "1              USA     56.938691    -154.182556  \n",
       "2              USA     60.904532    -161.420910  \n",
       "3              USA     60.904812    -161.227019  \n",
       "4              USA     54.132467    -165.785311  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Using Pandas API ###\n",
    "# Pandas accepts db connection as a reader \n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql.html\n",
    "import pandas as pd\n",
    "airlines_pd_df = pd.read_sql('select * from default.airports limit 20',connection)\n",
    "airlines_pd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.close() #don't forget to close the connection !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Read using SQLAlchemy\n",
    "##### https://docs.sqlalchemy.org/en/13/core/connections.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Create an engine and connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqlalchemy.engine.base.Connection"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#with SQLAlchemy\n",
    "from sqlalchemy import create_engine\n",
    "hive_host='mlamairesse-1.vpc.cloudera.com'\n",
    "hive_port='10000' #default 10000\n",
    "\n",
    "# Doc for connnection string : \n",
    "# engine = create_engine(\"hive://<kerberos-username>@<hive-host>:<hive-port>/<db-name>\",connect_args={'auth': 'KERBEROS','kerberos_service_name': 'hive'})\n",
    "\n",
    "engine = create_engine(\"hive://\"+hive_host+\":\"+hive_port+\"/default\",\n",
    "                     connect_args={'auth': 'KERBEROS','kerberos_service_name': 'hive'})\n",
    "connection = engine.connect()\n",
    "type(connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ADK', 'Adak', 'Adak', 'AK', 'USA', 51.87796389, -176.6460306)\n",
      "('AKK', 'Akhiok', 'Akhiok', 'AK', 'USA', 56.93869083, -154.1825556)\n",
      "('Z13', 'Akiachak', 'Akiachak', 'AK', 'USA', 60.90453167, -161.42091000000002)\n",
      "('AKI', 'Akiak', 'Akiak', 'AK', 'USA', 60.90481194, -161.2270189)\n",
      "('KQA', 'Akutan SPB', 'Akutan', 'AK', 'USA', 54.13246694, -165.7853111)\n",
      "('AUK', 'Alakanuk', 'Alakanuk', 'AK', 'USA', 62.68004417, -164.6599253)\n",
      "('5A8', 'Aleknagik', 'Aleknagik', 'AK', 'USA', 59.28256167, -158.6176725)\n",
      "('6A8', 'Allakaket', 'Allakaket', 'AK', 'USA', 66.55194444, -152.62222219999998)\n",
      "('BIG', 'Allen AAF', 'Delta Junction', 'AK', 'USA', 63.99454722, -145.7216417)\n",
      "('AFM', 'Ambler', 'Ambler', 'AK', 'USA', 67.10610472, -157.85362030000002)\n",
      "('AKP', 'Anaktuvuk Pass', 'Anaktuvuk Pass', 'AK', 'USA', 68.1343225, -151.74168)\n",
      "('AGN', 'Angoon SPB', 'Angoon', 'AK', 'USA', 57.50355528, -134.5850939)\n",
      "('ANI', 'Aniak', 'Aniak', 'AK', 'USA', 61.58159694, -159.5430428)\n",
      "('ANV', 'Anvik', 'Anvik', 'AK', 'USA', 62.64858333, -160.1898889)\n",
      "('ARC', 'Arctic Village', 'Arctic Village', 'AK', 'USA', 68.11608083, -145.5761114)\n",
      "('AKA', 'Atka', 'Atka', 'AK', 'USA', 52.22034833, -174.2063503)\n",
      "('4A2', 'Atmautluak', 'Atmautluak', 'AK', 'USA', 60.86674556, -162.2731389)\n",
      "('ATK', 'Atqasuk', 'Atqasuk', 'AK', 'USA', 70.46727611, -157.4357361)\n",
      "('BNF', 'Baranof Warm Springs SPB', 'Baranof Warm Springs', 'AK', 'USA', 57.08882583, -134.83314140000002)\n",
      "('BTI', 'Barter Island ', 'Kaktovik', 'AK', 'USA', 70.13390278, -143.5770444)\n"
     ]
    }
   ],
   "source": [
    "## basic reading \n",
    "data = connection.execute('select * from default.airports LIMIT 20')\n",
    "for row in data : \n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iata</th>\n",
       "      <th>airport</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADK</td>\n",
       "      <td>Adak</td>\n",
       "      <td>Adak</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>51.877964</td>\n",
       "      <td>-176.646031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AKK</td>\n",
       "      <td>Akhiok</td>\n",
       "      <td>Akhiok</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>56.938691</td>\n",
       "      <td>-154.182556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Z13</td>\n",
       "      <td>Akiachak</td>\n",
       "      <td>Akiachak</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>60.904532</td>\n",
       "      <td>-161.420910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AKI</td>\n",
       "      <td>Akiak</td>\n",
       "      <td>Akiak</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>60.904812</td>\n",
       "      <td>-161.227019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KQA</td>\n",
       "      <td>Akutan SPB</td>\n",
       "      <td>Akutan</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>54.132467</td>\n",
       "      <td>-165.785311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AUK</td>\n",
       "      <td>Alakanuk</td>\n",
       "      <td>Alakanuk</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>62.680044</td>\n",
       "      <td>-164.659925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5A8</td>\n",
       "      <td>Aleknagik</td>\n",
       "      <td>Aleknagik</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>59.282562</td>\n",
       "      <td>-158.617672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6A8</td>\n",
       "      <td>Allakaket</td>\n",
       "      <td>Allakaket</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>66.551944</td>\n",
       "      <td>-152.622222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BIG</td>\n",
       "      <td>Allen AAF</td>\n",
       "      <td>Delta Junction</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>63.994547</td>\n",
       "      <td>-145.721642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AFM</td>\n",
       "      <td>Ambler</td>\n",
       "      <td>Ambler</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>67.106105</td>\n",
       "      <td>-157.853620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>AKP</td>\n",
       "      <td>Anaktuvuk Pass</td>\n",
       "      <td>Anaktuvuk Pass</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>68.134322</td>\n",
       "      <td>-151.741680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>AGN</td>\n",
       "      <td>Angoon SPB</td>\n",
       "      <td>Angoon</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>57.503555</td>\n",
       "      <td>-134.585094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ANI</td>\n",
       "      <td>Aniak</td>\n",
       "      <td>Aniak</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>61.581597</td>\n",
       "      <td>-159.543043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ANV</td>\n",
       "      <td>Anvik</td>\n",
       "      <td>Anvik</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>62.648583</td>\n",
       "      <td>-160.189889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ARC</td>\n",
       "      <td>Arctic Village</td>\n",
       "      <td>Arctic Village</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>68.116081</td>\n",
       "      <td>-145.576111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>AKA</td>\n",
       "      <td>Atka</td>\n",
       "      <td>Atka</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>52.220348</td>\n",
       "      <td>-174.206350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4A2</td>\n",
       "      <td>Atmautluak</td>\n",
       "      <td>Atmautluak</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>60.866746</td>\n",
       "      <td>-162.273139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ATK</td>\n",
       "      <td>Atqasuk</td>\n",
       "      <td>Atqasuk</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>70.467276</td>\n",
       "      <td>-157.435736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BNF</td>\n",
       "      <td>Baranof Warm Springs SPB</td>\n",
       "      <td>Baranof Warm Springs</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>57.088826</td>\n",
       "      <td>-134.833141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>BTI</td>\n",
       "      <td>Barter Island</td>\n",
       "      <td>Kaktovik</td>\n",
       "      <td>AK</td>\n",
       "      <td>USA</td>\n",
       "      <td>70.133903</td>\n",
       "      <td>-143.577044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   iata                   airport                  city state country  \\\n",
       "0   ADK                      Adak                  Adak    AK     USA   \n",
       "1   AKK                    Akhiok                Akhiok    AK     USA   \n",
       "2   Z13                  Akiachak              Akiachak    AK     USA   \n",
       "3   AKI                     Akiak                 Akiak    AK     USA   \n",
       "4   KQA                Akutan SPB                Akutan    AK     USA   \n",
       "5   AUK                  Alakanuk              Alakanuk    AK     USA   \n",
       "6   5A8                 Aleknagik             Aleknagik    AK     USA   \n",
       "7   6A8                 Allakaket             Allakaket    AK     USA   \n",
       "8   BIG                 Allen AAF        Delta Junction    AK     USA   \n",
       "9   AFM                    Ambler                Ambler    AK     USA   \n",
       "10  AKP            Anaktuvuk Pass        Anaktuvuk Pass    AK     USA   \n",
       "11  AGN                Angoon SPB                Angoon    AK     USA   \n",
       "12  ANI                     Aniak                 Aniak    AK     USA   \n",
       "13  ANV                     Anvik                 Anvik    AK     USA   \n",
       "14  ARC            Arctic Village        Arctic Village    AK     USA   \n",
       "15  AKA                      Atka                  Atka    AK     USA   \n",
       "16  4A2                Atmautluak            Atmautluak    AK     USA   \n",
       "17  ATK                   Atqasuk               Atqasuk    AK     USA   \n",
       "18  BNF  Baranof Warm Springs SPB  Baranof Warm Springs    AK     USA   \n",
       "19  BTI            Barter Island               Kaktovik    AK     USA   \n",
       "\n",
       "          lat        long  \n",
       "0   51.877964 -176.646031  \n",
       "1   56.938691 -154.182556  \n",
       "2   60.904532 -161.420910  \n",
       "3   60.904812 -161.227019  \n",
       "4   54.132467 -165.785311  \n",
       "5   62.680044 -164.659925  \n",
       "6   59.282562 -158.617672  \n",
       "7   66.551944 -152.622222  \n",
       "8   63.994547 -145.721642  \n",
       "9   67.106105 -157.853620  \n",
       "10  68.134322 -151.741680  \n",
       "11  57.503555 -134.585094  \n",
       "12  61.581597 -159.543043  \n",
       "13  62.648583 -160.189889  \n",
       "14  68.116081 -145.576111  \n",
       "15  52.220348 -174.206350  \n",
       "16  60.866746 -162.273139  \n",
       "17  70.467276 -157.435736  \n",
       "18  57.088826 -134.833141  \n",
       "19  70.133903 -143.577044  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Using Pandas API ###\n",
    "# Pandas accepts db connection as a reader \n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql.html\n",
    "import pandas as pd\n",
    "airlines_pd_df = pd.read_sql('select * from default.airports limit 20',connection)\n",
    "airlines_pd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection.close() ### Don't forget to close the connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
